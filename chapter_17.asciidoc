Continuous Integration (CI)
---------------------------


As our site grows, it takes longer and longer to run all of our Functional
tests.  If this continues, the danger is that we're going to stop bothering.

Rather than let that happen, we can automate the running of functional tests
by setting up a "Continuous Integration" or CI server.  That way, in day-to-day
development, we can just run the FT that we're working on at that time, and
rely on the CI server to run all the test automatically, and let us know if
we've broken anything accidentally.  The unit tests should stay fast enough
that we can keep running them every few seconds.

The CI server of choice these days is called Jenkins. It's a bit Java, a bit
crashy, a bit ugly, but it's what everyone uses, and it has a great plugin
ecosystem, so let's get it up and running.


Installing Jenkins
~~~~~~~~~~~~~~~~~~

There are several hosted-CI services out there that essentially provide you
with a Jenkins server, ready to go.  I've come across Travis, Circle-CI,
ShiningPanda, and there are probably lots more.  I'm going to assume we're
going to install everything on a server we control.

NOTE: It's not a good idea to install Jenkins on the same server as our
staging or production servers.  Apart from anything else, we may want 
Jenkins to be able to reboot the staging server!

We'll install latest version from official jenkins apt repo, because the 
Ubuntu default still has a few annoying bugs with locale/unicode support,
and it also doesn't set itself up to listen on the public internet by default.

----
user@server:$ wget -q -O - http://pkg.jenkins-ci.org/debian/jenkins-ci.org.key | sudo apt-key add -
user@server:$ echo deb http://pkg.jenkins-ci.org/debian binary/ | sudo tee \
    /etc/apt/sources.list.d/jenkins.list
user@server:$ sudo apt-get update
user@server:$ sudo apt-get install jenkins
----

While we're at we'll install a few other dependencies:

----
user@server:$ sudo apt-get install git firefox python3 python-virtualenv xvfb
----

You should then be able to visit it at the URL for your server on port `8080`:

.A butler! How quaint...
image::images/jenkins_first_page.png["Jenkins' default welcome screen"]


Configuring Jenkins security
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The first thing we'll want to do is set up some authentication, so that
anyone can't just come and mess with our server:

* Manage Jenkins -> Configure Global Security -> Enable security
* Choose "Jenkins' own user database", "Matrix-based security"
* Disable all permissions for Anonymous
* And add a user for yourself
* The next screen offers you the option to create an account that matches that
  username, and set a password

.Locking it down...
image::images/jenkins_security_config.png["Jenkins' default welcome screen"]


Adding required plugins
^^^^^^^^^^^^^^^^^^^^^^^

* Manage Jenkins -> Manage Plugins -> Available
* We'll want the plugins for:
    * Git
    * ShiningPanda
    * Xvfb


.Installing plugins...
image::images/jenkins_installing_plugins.png["Jenkins installing plugins"]

Restart afterwards - either using the tick-box on that last screen, or
from the command-line with a `sudo restart jenkins`.


Telling jenkins where to find Python 3
++++++++++++++++++++++++++++++++++++++

* Manage Jenkins -> Configure System
* Python -> Python installations -> Add Python 

.Where did I leave that Python?
image::images/jenkins_adding_python3.png["Adding Python 3"]


Setting up our project
~~~~~~~~~~~~~~~~~~~~~~

Now we've got the basic jenkins configured, let's set up our project.

* New Job -> Build a free-style software project 
* Add the Git repo

.Get it from git
image::images/jenkins_set_git_repo.png["Setting the git repo"]

* Set it to poll every hour (the help text here is helpful)

.Poll Github for changes
image::images/jenkins_poll_scm.png["Config polling github"]

* Run the tests inside a Python 3 virtualenv
* Run the unit tests and functional tests separately

.Virtualenv build steps
image::images/jenkins_build_in_virtualenv.png["Adding Python 3"]



First build!
~~~~~~~~~~~~

Hit "Build Now!", then go and take a look at the "Console Output". You
should see something like this:

----
Started by user harry
Building in workspace /var/lib/jenkins/jobs/Superlists/workspace
Fetching changes from the remote Git repository
Fetching upstream changes from https://github.com/hjwp/book-example.git
Checking out Revision d515acebf7e173f165ce713b30295a4a6ee17c07 (origin/master)
[workspace] $ /bin/sh -xe /tmp/shiningpanda7260707941304155464.sh
+ pip install -r requirements.txt
Requirement already satisfied (use --upgrade to upgrade): Django==1.6 in
/var/lib/jenkins/shiningpanda/jobs/ddc1aed1/virtualenvs/d41d8cd9/lib/python3.3/site-packages
(from -r requirements.txt (line 1))
Downloading/unpacking South==0.8.2 (from -r requirements.txt (line 2))
  Running setup.py egg_info for package South
    
Requirement already satisfied (use --upgrade to upgrade): gunicorn==17.5 in
/var/lib/jenkins/shiningpanda/jobs/ddc1aed1/virtualenvs/d41d8cd9/lib/python3.3/site-packages
(from -r requirements.txt (line 3))
Downloading/unpacking requests==2.0.0 (from -r requirements.txt (line 4))
  Running setup.py egg_info for package requests
    
Installing collected packages: South, requests
  Running setup.py install for South
    
  Running setup.py install for requests
    
Successfully installed South requests
Cleaning up...
+ python manage.py test lists accounts
...................................................
 ---------------------------------------------------------------------
Ran 51 tests in 0.323s

OK
Creating test database for alias 'default'...
Destroying test database for alias 'default'...
+ python manage.py test functional_tests
ImportError: No module named 'selenium'
Build step 'Virtualenv Builder' marked build as failure
----

Ah.  We need selenium in our virtualenv.

TIP: Some people like to use a file called 'test-requirements.txt' to specify 
packages that are needed for the tests, but not the main app.

Let's add a manual installation of selenium to our build steps:

    pip install -r requirements.txt
    pip install selenium
    python manage.py test accounts lists
    python manage.py test functional_tests

Now what?

----
  File
  "/var/lib/jenkins/shiningpanda/jobs/ddc1aed1/virtualenvs/d41d8cd9/lib/python3.3/site-packages/selenium/webdriver/firefox/firefox_binary.py",
  line 100, in _wait_until_connectable
    self._get_firefox_output())
selenium.common.exceptions.WebDriverException: Message: 'The browser appears to
have exited before we could connect. The output was: b"\\n(process:19757):
GLib-CRITICAL **: g_slice_set_config: assertion \'sys_page_size == 0\'
failed\\nError: no display specified\\n"' 
----


Setting up a virtual display so the FTs can run headless
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

As you can see from the traceback, Firefox is unable to start because the
server doesn't have a display.

There are two ways to deal with this problem. The first is to switch to using
a headless browser, like PhantomJS or SlimerJS.  Those tools definitely have
their place -- they're faster, for one thing -- but they also have
disadvantages.  The first is that they're not "real" web browsers, so you can't
be sure you're going to catch all the strange quirks and behaviours of the
actual browsers your users use.  The second is that they behave quite
differently inside Selenium, and will require substantial amounts of re-writing
of FT code.

TIP: I would look into using headless browsers as a "dev-only" tool, to speed
up the running of FTs on the developer's machine, while the tests on the CI
server use actual browsers.

The alternative is to set up a virtual display:  we get the server to pretend
it has a screen attached to it, so Firefox runs happily. There's a few tools
out there to do this, we'll use one called "Xvfb" (X Virtual Framebuffer) 
because it's easy to install and use, and because it has a convenient Jenkins
plug-in

TIP: Check out https://pypi.python.org/pypi/PyVirtualDisplay[pyvirtualdisplay]
as a way of controlling virtual displays from Python.

We go back to our project and hit "Configure" again, then find the section
called "Build Environment".  Using the virtual display is as simple as
ticking the box marked "Start Xvfb before the build, and shut it down after.":

.Sometimes config is easy
image::images/jenkins_start_xvfb.png["Tickbox saying we want Xvfb"]


The build does much better now,

----
[...]
Xvfb starting$ /usr/bin/Xvfb :2 -screen 0 1024x768x24 -fbdir
/var/lib/jenkins/2013-11-04_03-27-221510012427739470928xvfb
[...]
+ python manage.py test lists accounts
...................................................
 ---------------------------------------------------------------------
Ran 51 tests in 0.410s

OK
Creating test database for alias 'default'...
Destroying test database for alias 'default'...
+ pip install selenium
Requirement already satisfied (use --upgrade to upgrade): selenium in
/var/lib/jenkins/shiningpanda/jobs/ddc1aed1/virtualenvs/d41d8cd9/lib/python3.3/site-packages
Cleaning up...

+ python manage.py test functional_tests
.....F.
======================================================================
FAIL: test_logged_in_users_lists_are_saved_as_my_lists
(functional_tests.test_my_lists.MyListsTest)
 ---------------------------------------------------------------------
Traceback (most recent call last):
  File "/var/lib/jenkins/jobs/Superlists chapter 17/workspace/functional_tests/test_my_lists.py", line 44, in test_logged_in_users_lists_are_saved_as_my_lists
    self.assertEqual(self.browser.current_url, first_list_url)
AssertionError: 'http://localhost:8081/accounts/edith@email.com/' != 'http://localhost:8081/lists/1/'
- http://localhost:8081/accounts/edith@email.com/
+ http://localhost:8081/lists/1/

 ---------------------------------------------------------------------
Ran 7 tests in 89.275s

FAILED (errors=1)
Creating test database for alias 'default'...
[{'secure': False, 'domain': 'localhost', 'name': 'sessionid', 'expiry':
1920011311, 'path': '/', 'value': 'a8d8bbde33nreq6gihw8a7r1cc8bf02k'}]
Destroying test database for alias 'default'...
Build step 'Virtualenv Builder' marked build as failure
Xvfb stopping
Finished: FAILURE
----

Pretty close!  To debug that failure, we'll need screenshots though.

NOTE: As we'll see, this error is due to a race condition, which means it's
not always reproducible.  You may see a different error, or none at all. In
any case, the tools below for taking screenshots and dealing with race
conditions will come in useful. Read on!


Taking screenshots
~~~~~~~~~~~~~~~~~~

To be able to debug unexpected failures that happen on a remote PC, it
would be good to see a picture of the screen at the moment of the failure,
and maybe also a dump of the HTML of the page.  We can do that using some
custom logic in our FT class teardown. We have to do a bit of introspection of
`unittest` internals, but his will work:

[role="sourcecode"]
.functional_tests/base.py
[source,python]
----
    def tearDown(self):
        if not self._outcomeForDoCleanups.success:
            self.take_screenshot()
            self.dump_html()

        self.browser.quit()
        super().tearDown()
----

Then we use some selenium methods, `get_sreenshot_as_file` and
`browser.page_source` for our image and HTML dumps:

[role="sourcecode"]
.functional_tests/base.py
[source,python]
----
    def take_screenshot(self):
        filename = 'seleniumscreenshot-{}.png'.format(self._get_filename())
        print('screenshotting to', filename)
        self.browser.get_screenshot_as_file(filename)


    def dump_html(self):
        filename = 'seleniumhtml-{}.html'.format(self._get_filename())
        print('dumping page HTML to', filename)
        with open(filename, 'w') as f:
            f.write(self.browser.page_source)
----

And finally here's a way of generating a filename, which includes the
name of the test and its class, as well as a timestamp:

[role="sourcecode"]
.functional_tests/base.py
[source,python]
----
    def _get_filename(self):
        timestamp = datetime.now().isoformat().replace(':', '.')
        return '{}.{}-{}'.format(
            self.__class__.__name__, self._testMethodName, timestamp
        )
----

When we re-run the build on Jenkins, we see this:

----
screenshotting to
seleniumscreenshot-MyListsTest.test_logged_in_users_lists_are_saved_as_my_lists-2013-11-05T05.02.03.634488.png

dumping page HTML to
seleniumhtml-MyListsTest.test_logged_in_users_lists_are_saved_as_my_lists-2013-11-05T05.02.03.695734.html
----


We can go and visit these in the "workspace", which is the folder which jenkins
uses to store our source code and run the tests in:

.Visiting the project workspace
image::images/jenkins_workspace_with_screenshots.png["worspace files including screenshot"]

And then we look at the screenshot:


.Screenshot looking normal
image::images/jenkins_screenshot_example.png["Screenshot of site page"]


Well, that didn't help much


A common Selenium problem: race conditions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Whenever you see an inexplicable failure in a Selenium test, one of the most
likely explanations is a hidden race condition. Let's look at the line that
failed:

        # She sees that her list is in there, named according to its
        # first list item
        self.browser.find_element_by_link_text('Reticulate splines').click()
        self.assertEqual(self.browser.current_url, first_list_url)

Immediately after we click the "Reticulate splines" link, we ask selenium
to check whether the current URL matches the URL for our first list.  But
it doesn't:

----
AssertionError: 'http://localhost:8081/accounts/edith@email.com/' !=
'http://localhost:8081/lists/1/'
----

It looks like the current URL is still the URL of the "my lists" page.  What's
going on?

Do you remember that we set an `implicitly_wait` on the browser, way back in
chapter 2?  Well, that implicitly_wait applies to any calls to any of the 
Selenium `find_element_` calls, but it doesn't apply to `browser.current_url`.
Selenium doesn't "wait" when you tell it to click an element, so what's happened
is that the browser hasn't finished loading the new page yet, so `current_url`
is still the old page.  We need to use some more wait code, like we did for the
various Persona pages.

At this point it's time for a "wait for" helper function. To see how this
is going to work, it helps to see how I expect to use it:


[role="sourcecode"]
.functional_tests/test_my_lists.py
[source,python]
----
    # She sees that her list is in there, named according to its
    # first list item
    self.browser.find_element_by_link_text('Reticulate splines').click()
    self.wait_for(
        lambda: self.assertEqual(self.browser.current_url, first_list_url)
    )
----

We're going to take our `assertEqual` call and turn it into a lambda function,
then pass it into our `wait_for` helper.

[role="sourcecode"]
.functional_tests/base.py
[source,python]
----
    def wait_for(self, function_with_assertion, timeout=DEFAULT_WAIT):
        start_time = time.time()
        while True:
            try:
                return function_with_assertion()
            except AssertionError:
                if time.time() - start_time > timeout:
                    raise
----

`wait_for` then tries to execute that function, but instead of letting the 
test fail if the assertion fails, it catches the `AssertionError` that
`assertEqual` would ordinarily raise, and loops around inside the `while True`
retrying it, until a given timeout.

I've added the timeout there as an optional argument, and I'm basing it on 
a constant we'll add to 'base.py'.  We'll also use it in our original 
`implicitly_wait`:


[role="sourcecode"]
.functional_tests/base.py
[source,python]
----
[...]
import time

DEFAULT_WAIT = 3


class FunctionalTest(LiveServerTestCase):

    [...]

    def setUp(self):
        self.browser = webdriver.Firefox()
        self.browser.implicitly_wait(DEFAULT_WAIT)
----


Now we can re-run the test to confirm it still works:

[subs="specialcharacters,macros"]
----
$ pass:quotes[*python3 manage.py test functional_tests.MyListsTest*]
Creating test database for alias 'default'...
.
 ---------------------------------------------------------------------
Ran 1 test in 9.594s
----

And, just to be sure, we'll deliberately break our test to see it fail
too:


[role="sourcecode"]
.functional_tests/test_my_lists.py
[source,python]
----
    self.wait_for(
        lambda: self.assertEqual(self.browser.current_url, 'barf')
    )
----

Sure enough, that gives:

----
AssertionError: 'http://localhost:8081/lists/1/' != 'barf'
----

And we see it pause on the page for 3 seconds.  Let's revert that last 
change, and the commit our changes:


[subs="specialcharacters,quotes"]
----
$ *git status*
[...]
#   modified:   functional_tests/base.py
#   modified:   functional_tests/test_my_lists.py
$ *git add Functional tests*
$ *git diff --staged*
$ *git commit -m"use wait_for function for URL checks in my_lists"*
$ *git push* # send our changes up to github
----

Then we can re-run the build on Jenkins using "Build now", and confirm it now
works:

.The outlook is looking better
image::images/jenkins_build_starting_to_look_better["Build showing a recent pass and sun-peeking-through-clouds logo"]

Jenkins uses blue to indicate passing builds rather than green, which is a bit
disappointing, but look at the sun peeking through the clouds!  That's cheery!
It's an indicator of a moving average ratio of passing builds to failing
builds.  Things are looking up!


Running our Qunit JavaScript tests in Jenkins
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

There's a set of tests we almost forgot -- the JavaScript tests. Currently
our "test runner" is an actual web browser.  To get Jenkins to run them, we
need a command-line test runner.  Here's a chance to use PhantomJS.

Installing node
^^^^^^^^^^^^^^^

It's time to stop pretending we're not in the JavaScript game.  We're doing
web development.  That means we do JavaScript.  That means we're going to end
up with node.js on our computers.  It's just the way it has to be.

Follow the instructions on the http://nodejs.org/download/[node.js download
page]. There are installers for Windows and Mac, and repositories for popular
Linux distros.

Once we have node, we can install phantom:

[subs="specialcharacters,quotes"]
----
$ *npm install -g phantomjs*  # the -g means "system-wide". May need sudo.
----

Next we pull down the Qunit test runner.  You can either download it
manually from https://github.com/jquery/qunit/tree/master/addons/phantomjs 
or, if you feel like it:

----
$ *npm install -g bower*  # bower is a front-end web-dev package manager
$ *cd /tmp*
$ *bower install qunit*
bower qunit#*                   cached git://github.com/jquery/qunit.git#1.12.0
bower qunit#*                 validate 1.12.0 against git://github.com/jquery/qunit.git#*
bower qunit#~1.12.0            install qunit#1.12.0
# bower puts everything into a folder called "bower_components".
# we dig out our test runner from in there:
$ *cp bower_components/qunit/addons/phantomjs/runner.js /workspace/superlists/superlists/static/tests/
----

That's not much simpler, but I just wanted to show you bower.  I won't go into
it in this book, but it's definitely worth looking into.  Suffice to say I'm
misusing it horrendously here. In any case, you should end up with this:

[subs="specialcharacters,quotes"]
----
$ *tree superlists/static/tests/*
superlists/static/tests/
├── qunit.css
├── qunit.js
├── runner.js
└── sinon.js
----

Now let's try it out:

[subs="specialcharacters,quotes"]
----
$ *phantomjs superlists/static/tests/runner.js lists/static/tests/tests.html*
Took 24ms to run 2 tests. 2 passed, 0 failed.
$ *phantomjs superlists/static/tests/runner.js accounts/static/tests/tests.html*
Took 29ms to run 11 tests. 11 passed, 0 failed.
----

And just to be sure, let's deliberately break something:

[role="sourcecode"]
.lists/static/list.js
[source,javascript]
----
$('input').on('keypress', function () {
    //$('.has-error').hide();
});
----

Sure enough:

[subs="specialcharacters,quotes"]
----
$ *phantomjs superlists/static/tests/runner.js lists/static/tests/tests.html*
Test failed: undefined: errors should be hidden on keypress
    Failed assertion: expected: false, but was: true
    at file:///worskpace/superlists/superlists/static/tests/qunit.js:556
    at file:///worskpace/superlists/lists/static/tests/tests.html:25
    at file:///worskpace/superlists/superlists/static/tests/qunit.js:203
    at file:///worskpace/superlists/superlists/static/tests/qunit.js:361
    at process (file:///worskpace/superlists/superlists/static/tests/qunit.js:1453)
    at file:///worskpace/superlists/superlists/static/tests/qunit.js:479
Took 27ms to run 2 tests. 1 passed, 1 failed.
----

All right!  Let's unbreak that, commit and push it, and then add it to our
Jenkins build:

[subs="specialcharacters,quotes"]
----
$ *git add superlists/static/tests/runner.js
$ *git commit -m"Add phantomjs test runner for javascript tests"*
$ *git push* 
----

.Add a build step for our JavaScript unit tests
image::images/jenkins_add_phantomjs_build_step.png["Jenkins' default welcome screen"]

You'll also need to install phantomjs on the server.

----
sudo add-apt-repository -y ppa:chris-lea/node.js
sudo apt-get update
sudo apt-get install nodejs
sudo npm install -g phantomjs
----

And there we are!  A complete CI build featuring all of our tests!


----
Started by user harry
Building in workspace /var/lib/jenkins/jobs/Superlists chapter 17/workspace
Fetching changes from the remote Git repository
Fetching upstream changes from https://github.com/hjwp/book-example.git
Checking out Revision 936a484038194b289312ff62f10d24e6a054fb29 (origin/chapter_17)
Xvfb starting$ /usr/bin/Xvfb :1 -screen 0 1024x768x24 -fbdir /var/lib/jenkins/2013-11-06_11-08-026223733569337356081xvfb
[workspace] $ /bin/sh -xe /tmp/shiningpanda7092102504259037999.sh

+ pip install -r requirements.txt
[...]

+ python manage.py test lists
.................................
 ---------------------------------------------------------------------
Ran 33 tests in 0.229s

OK
Creating test database for alias 'default'...
Destroying test database for alias 'default'...

+ python manage.py test accounts
..................
 ---------------------------------------------------------------------
Ran 18 tests in 0.078s

OK
Creating test database for alias 'default'...
Destroying test database for alias 'default'...

[workspace] $ /bin/sh -xe /tmp/hudson2967478575201471277.sh
+ phantomjs superlists/static/tests/runner.js lists/static/tests/tests.html
Took 32ms to run 2 tests. 2 passed, 0 failed.
+ phantomjs superlists/static/tests/runner.js accounts/static/tests/tests.html
Took 47ms to run 11 tests. 11 passed, 0 failed.

[workspace] $ /bin/sh -xe /tmp/shiningpanda7526089957247195819.sh
+ pip install selenium
Requirement already satisfied (use --upgrade to upgrade): selenium in /var/lib/jenkins/shiningpanda/jobs/c14c656b/virtualenvs/d41d8cd9/lib/python3.3/site-packages

Cleaning up...
[workspace] $ /bin/sh -xe /tmp/shiningpanda2420240268202055029.sh
+ python manage.py test functional_tests
.......
 ---------------------------------------------------------------------
Ran 7 tests in 76.804s

OK
----

Nice to know that, no matter how lazy I get about running the full test suite
on my own machine, the CI server will catch me.  Another one of the testing
goat's agents in cyberspace, watching over us...


.Tips on CI and Selenium best practices
*******************************************************************************
Set up CI as soon as possible for your project::
    As soon as your functional tests take more than a few seconds to run,
    you'll find yourself avoiding running them all. Give this job to a CI
    server, to make sure that all your tests are getting run somewhere.

Set up screenshots and HTML dumps for failures::
    Debugging test failures is easier if you can see what the page looked
    at when the failure occurs.  This is particularly useful for debugging
    CI failures, but it's also very useful for tests that you run locally.

Use waits in Selenium tests::
    Selenium's `implicitly_wait` only applies to uses of its `find_element` 
    functions, and even that can be unreliable (it can find an element that's
    still on the old page).  Build a `wait_for` helper function, and alternate
    between actions on the site, and then some sort of wait to see that they've
    taken effect.
*******************************************************************************


TODO: Appendix on switching to PhantomJS?  

TODO: More discussion of do-something / wait for alternation as best practice?

TODO: Something about jUnit XML output using py.test?
