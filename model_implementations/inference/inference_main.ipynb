{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1361/Book-TDD-Web-Dev-Python/blob/master/model_implementations/inference/inference_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za04NM_dli1g"
      },
      "outputs": [],
      "source": [
        "# @title [RUN ME] Set up detectron, mount drive, import utility functions.\n",
        "# Suppress the output of this cell\n",
        "%%capture\n",
        "\n",
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Clone into the BovEye github repo, access token expires 04/01/24\n",
        "!git clone https://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@github.com/Boveye/Boveye.git\n",
        "\n",
        "import sys, os, distutils.core\n",
        "# Clone into Detectron2 Github repo, install Detectron2 and dependencies\n",
        "!python -m pip install pyyaml==5.1\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities.\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "# Properly install detectron2. (Please do not install twice in both ways)\n",
        "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "\n",
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)\n",
        "\n",
        "# Setup detectron2 logger\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "import numpy as np;\n",
        "import time\n",
        "import os, json, cv2, random, locale, csv, shutil\n",
        "from IPython.display import Image, display\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog, Metadata\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "\n",
        "# Install dependencies and import utility functions\n",
        "# Ensure you import inference AFTER installing detectron\n",
        "sys.path.insert(0, '/content/Boveye/model_implementations/inference/inference_utils')\n",
        "!pip install -r '/content/Boveye/model_implementations/inference/requirements.txt'\n",
        "import split\n",
        "import inference\n",
        "from inference_paths import Paths\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-4BH8ffCix5"
      },
      "source": [
        "### Running the following will accomplish:\n",
        "\n",
        "\n",
        "* Pen segmentation and standardization\n",
        "* Inference on standardized tiles\n",
        "* Save prediction CSV, reconstructed to per pen basis (if desired)\n",
        "* Save pen images for qa (if desired)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMmX9v7A0v4g"
      },
      "outputs": [],
      "source": [
        "# Update if Boveye Engineering folder is referenced differently in your drive\n",
        "root = '/content/drive/MyDrive/Boveye/1_Engineering'\n",
        "\n",
        "# See 'boveye_image_identifiers.xlsx' in Google Drive for reference\n",
        "source = '0'\n",
        "\n",
        "# use 'raw' if using script processing, update if using manually processed\n",
        "version = 'raw'\n",
        "\n",
        "paths = Paths(root, source, version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNDNo0s1fOiA"
      },
      "outputs": [],
      "source": [
        "# Segment pens from 8bit tif. Returns dict of image numpy arrays.\n",
        "segmented_images = split.segment_pens(paths.shp, paths.tif)\n",
        "\n",
        "if version == 'raw':\n",
        "    params = [2, 98, 2] # [num_std, top_cut_percentile, bot_cut_percentile] # [2,98,2] has been producing best results\n",
        "    segmented_images = split.process_segmented_images(segmented_images,\n",
        "                                                      num_std=params[0],\n",
        "                                                      max_percentile=params[1],\n",
        "                                                      min_percentile=params[2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize image preprocessing\n",
        "keys = list(segmented_images.keys())  # Get all keys from the dictionary\n",
        "selected_keys = random.sample(keys, 3)  # Randomly select 3 keys\n",
        "\n",
        "# Display the images for the selected keys\n",
        "for key in selected_keys:\n",
        "    print(f\"Displaying image for key: {key}\")\n",
        "    display(segmented_images[key])"
      ],
      "metadata": {
        "id": "6bW-2GKyNK4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsVti_UpfSaO",
        "outputId": "302c1c50-851e-41bb-eea6-f4771d8bd689",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padding & Standardizing Images.\n",
            "     1099 tiles created from 161 pens.\n",
            "     Standardization Complete.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Set tile size. Results in [size, size, 3] numpy arrays of padded images.\n",
        "# Recommend powers of two. Recommend 128 for 30cm, 32 for 50cm.\n",
        "size = 128\n",
        "\n",
        "# Make image tiles. Returns dict with padded numpy arrays and metadata\n",
        "padded_tiles = split.standardize_image_size(segmented_images, size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_N3PcKWfUel",
        "outputId": "047a3d18-5c8f-42b1-96e3-16080ccf4d63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.common.config:Loading config /content/detectron2/detectron2/model_zoo/configs/COCO-Detection/../Base-RetinaNet.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config built.\n",
            "  Weights: /content/drive/MyDrive/Boveye/1_Engineering/9_testing/7_model_weights/current_weights.pth\n",
            "  Output Directory: ./output\n",
            "  Number of Classes: 2\n",
            "  Detection Threshold: 0.4\n",
            "  NMS Threshold: 0.4\n",
            "  Max Detections: 1000\n",
            "Beginning Inference and Output.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Inference complete on 50 of 1099\n",
            "    Detected 808 cattle and counting...\n",
            "\n",
            "  Inference complete on 100 of 1099\n",
            "    Detected 1204 cattle and counting...\n",
            "\n",
            "  Inference complete on 150 of 1099\n",
            "    Detected 2000 cattle and counting...\n",
            "\n",
            "  Inference complete on 200 of 1099\n",
            "    Detected 2270 cattle and counting...\n",
            "\n",
            "  Inference complete on 250 of 1099\n",
            "    Detected 2595 cattle and counting...\n",
            "\n",
            "  Inference complete on 300 of 1099\n",
            "    Detected 2810 cattle and counting...\n",
            "\n",
            "  Inference complete on 350 of 1099\n",
            "    Detected 3120 cattle and counting...\n",
            "\n",
            "  Inference complete on 400 of 1099\n",
            "    Detected 3354 cattle and counting...\n",
            "\n",
            "  Inference complete on 450 of 1099\n",
            "    Detected 3589 cattle and counting...\n",
            "\n",
            "  Inference complete on 500 of 1099\n",
            "    Detected 3932 cattle and counting...\n",
            "\n",
            "  Inference complete on 550 of 1099\n",
            "    Detected 4222 cattle and counting...\n",
            "\n",
            "  Inference complete on 600 of 1099\n",
            "    Detected 4549 cattle and counting...\n",
            "\n",
            "  Inference complete on 650 of 1099\n",
            "    Detected 5105 cattle and counting...\n",
            "\n",
            "  Inference complete on 700 of 1099\n",
            "    Detected 5310 cattle and counting...\n",
            "\n",
            "  Inference complete on 750 of 1099\n",
            "    Detected 5421 cattle and counting...\n",
            "\n",
            "  Inference complete on 800 of 1099\n",
            "    Detected 5615 cattle and counting...\n",
            "\n",
            "  Inference complete on 850 of 1099\n",
            "    Detected 5724 cattle and counting...\n",
            "\n",
            "  Inference complete on 900 of 1099\n",
            "    Detected 5882 cattle and counting...\n",
            "\n",
            "  Inference complete on 950 of 1099\n",
            "    Detected 6996 cattle and counting...\n",
            "\n",
            "  Inference complete on 1000 of 1099\n",
            "    Detected 7690 cattle and counting...\n",
            "\n",
            "  Inference complete on 1050 of 1099\n",
            "    Detected 9014 cattle and counting...\n",
            "\n",
            "Inference complete on 1099, total detections: 11326. Runtime: 99.66138172149658\n"
          ]
        }
      ],
      "source": [
        "# Set NMS for inference, 0.4 is default. Higher NMS = more detections.\n",
        "nms = 0.4\n",
        "\n",
        "# Set Threshold for inference, 0.4 is default. Lower thresh = more detections.\n",
        "thresh = 0.4\n",
        "\n",
        "# Inference on standardized tiles. Predictions are output on a per pen basis.\n",
        "predictions = inference.run(paths.weights, padded_tiles, nms, thresh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEfpBco2Gzzr"
      },
      "outputs": [],
      "source": [
        "# (If desired) Save predictions to CSV in Google Drive.\n",
        "if version == 'raw':\n",
        "    paths.set_output(nms, thresh, size, params)\n",
        "else:\n",
        "    paths.set_output(nms, thresh, size)\n",
        "\n",
        "predictions.to_csv(paths.out_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNubQo0ffBqb",
        "outputId": "d2f23b26-4f41-4b15-b3ca-ab761978b6d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pen images to /content/drive/MyDrive/Boveye/1_Engineering/9_testing/2_quality_assurance/3_pen_ims/0\n"
          ]
        }
      ],
      "source": [
        "# Save pen images in Google Drive for QA (if desired, deletes previous version)\n",
        "split.save_pen_images(segmented_images, paths.qa_ims)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}