
== Chapter 2

.Useful TDD concepts
*******************************************************************************
User story::
    A description of how the application will work from the point of view
    of the user.  Used to structure a Functional test.

Expected failure::
    When a test fails in the way that we expected it to.

*******************************************************************************


== chapter_03

.Useful commands and concepts
*******************************************************************************
Running the Django dev server::
    *`python3 manage.py runserver`*

Running the functional tests::
    *`python3 functional_tests.py`*

Running the unit tests::
    *`python3 manage.py test`*

The unit test / code cycle::
    * Run the unit tests in the terminal
    * Make a minimal code change in the editor
    * Repeat!

*******************************************************************************

== chapter_04 

[[Double-Loop-TDD-diagram]]
.The TDD process with Functional and Unit tests
image::images/twdp_0404.png[A flowchart showing functional tests as the overall cycle, and unit tests helping to code]


== chapter_05 

.Useful TDD concepts
*******************************************************************************

Regression::
    When new code breaks some aspect of the application which used to work.

Unexpected failure::
    When a test fails in a way we weren't expecting.  This either means that
    we've made a mistake in our tests, or that the tests have helped us find
    a regression, and we need to fix something in our code.

Red / Green / Refactor::
    Another way of describing the TDD process. Write a test and see it fail
    (Red), write some code to get it to pass (Green), then Refactor to improve
    the implementation.

Triangulation::
    Adding a test case with a new specific example for some existing code, to
    justify generalising the implementation (which may be a "cheat" until that
    point).

3 strikes and refactor::
    A rule of thumb for when to remove duplication from code.

The scratchpad to-do list::
    A place to write down things that occur to us as we're coding, so that
    we can finish up what we're doing and come back to them later.

*******************************************************************************


== chapter_06 

.Useful TDD concepts
*******************************************************************************

Test isolation::
    Different tests shouldn't affect one another.  This means we need to
    reset any permanent state at the end of each test. Django's test runner
    helps us do this by creating a test database, which it wipes clean in
    between each test.

The Testing Goat vs Refactoring Cat::
    Our natural urge is often to dive in and fix everything at once... but if
    we're not careful, we'll end up like Refactoring Cat, in a situation with
    loads of changes to our code and nothing working.  The Testing Goat
    encourages us to take on step at a time, and go from working state to
    working state.

*******************************************************************************

* Add YAGNI?

== chapter_07 

.Recap: On testing design and layout
*******************************************************************************

The short answer is: you shouldn't write tests for design and layout.  It's too
much like testing a constant, and any tests you write are likely to be brittle.

With that said, the 'implementation' of design and layout involves something 
quite tricky: CSS, and static files.   As a result, it is valuable to have some
kind of minimal "smoke test" which checks that your static files and CSS are
working.  As we'll see in the next chapter, it can help pick up problems when
you deploy your code to production.

Similarly, if a particular piece of styling required a lot of client-side 
JavaScript code to get it to work (dynamic resizing is one I've spent a bit
of time on), you'll definitely want some tests for that.

So be aware that this is a dangerous area.  Try and write the minimal tests 
that will give you confidence that your design and layout is working, without
testing 'what' it actually is.  Try and leave yourself in a position where you
can freely make changes to the design and layout, without having to go back and
adjust tests all the time.

*******************************************************************************


== chapter_08 

.Test-Driving server configuration and deployment 
*******************************************************************************

Tests take some of the uncertainty out of deployment::
    As developers, server administration is always "fun", by which I mean, a
    process full of uncertainty and surprises. My aim during this chapters was
    to show a functional test suite can take some of the uncertainty out of the
    process.  

Typical pain points - database, static files, dependencies, custom settings::
    The things that you need to keep an eye out on any deployment include
    your database configuration, static files, software dependencies, and
    custom settings that differ between development and production.  You'll
    need to think through each of these for your own deployments.

Tests allow us to experiment::
    Whenever we make a change to our server configuration, we can re-run the
    test suite, and be confident that everything works as well as it did
    before.  It allows us to experiment with our setup with less fear.

*******************************************************************************



== chapter_09 

.Automated deployments
*******************************************************************************

Fabric::
    Fabric lets you run commands on servers from inside Python scripts. This
    is a great tool for automating server admin tasks.

Idempotency::
    If your deployment script is deploying to existing servers, you need to
    design them so that they work against a fresh installation 'and' against
    a server that's already configured.

Keep config files under source control::
    Make sure your only copy of a config file isn't on the server!  They
    are critical to your application, and should be under version control
    like anything else.

Automating provisioning::
    Ultimately, 'everything' should be automated, and that includes spinning up
    brand new servers and ensuring they have all the right software installed.
    This will involve interacting with the API of your hosting provider.

Configuration management tools::
    Fabric is very flexible, but its logic is still based on scripting. More
    advanced tools take a more "declarative" approach, and can make your life
    even easier.  Ansible and Vagrant are two worth checking out, but there 
    are many more (Chef, Puppet, Salt, Juju...)
    
*******************************************************************************


== chapter_10 


.Tips on organising tests and refactoring
*******************************************************************************
Use a tests folder::
    Just as you use multiple files to hold your application code, you should
    split your tests out into multiple files.
    +
    * Use a folder called 'tests', adding a '__init__.py' which imports all
      test classes
    * For functional test, group them into tests for a particular feature or
      user story
    * For unit tests, you want a separate test file for each tested source code
      file. For Django, that's typically 'test_models.py', 'test_views.py',
      'test_forms.py'
    * Have at least a placeholder test for *every* function and class

Don't forget the "Refactor" in "Red, Green, Refactor"::
    The whole point of having tests is to allow you to refactor your code!
    Use them, and make your code as clean as you can.  

Don't refactor against failing tests::
    * In general!
    * But the FT you're currently working on doesn't count.
    * You can occasionally put a skip on a test which is testing something you
      haven't written yet.  
    * More commonly, make a note of the refactor you want to do, finish what
      you're working on, and do the refactor a little later, when you're back
      to a working state
    * Don't forget to remove any skips before you commit your code! You should
      always review your diffs line by line to catch things like this.
*******************************************************************************


== chapter_11 

.Tips
*******************************************************************************
Thin views::
    If you find yourself looking at complex views, and having to write a lot of
    tests for them, it's time to start thinking about whether that logic could
    be moved elsewhere: possibly to a form, like we've done here.  
    +
    +
    Another possible place would be a custom method on the model class.  And --
    once the complexity of the app demands it -- out of Django-specific files
    and into your own classes and functions, that capture your core business
    logic.

Each test should test one thing::
    The heuristic is to be suspicious if there's more than one assertion in a
    test. Sometimes two assertions are closely related, so they belong together.
    But often your first draft of a test ends up testing multiple behaviours,
    and it's worth re-writing it as several tests. Helper functions can keep
    them from getting too bloated.
*******************************************************************************



== chapter_12 

.Recap: what to test in views
******************************************************************************

[role="sourcecode skipme"]
[source,python]
.Partial listing show all view test + assertions
----
class ListViewTest(TestCase):
    def test_uses_list_template(self):
        response = self.client.get('/lists/%d/' % (list_.id,)) #<1>
        self.assertTemplateUsed(response, 'list.html') #<2>
    def test_passes_correct_list_to_template(self):
        self.assertEqual(response.context['list'], correct_list) #<3>
    def test_displays_item_form(self):
        self.assertIsInstance(response.context['form'], ExistingListItemForm) #<4>
        self.assertContains(response, 'name="text"')
        self.assertContains(response, 'name="text"')
    def test_displays_only_items_for_that_list(self):
        self.assertContains(response, 'itemey 1') #<5>
        self.assertContains(response, 'itemey 2') #<5>
        self.assertNotContains(response, 'other list item 1') #<5>
    def test_can_save_a_POST_request_to_an_existing_list(self):
        self.assertEqual(Item.objects.count(), 1) #<6>
        self.assertEqual(new_item.text, 'A new item for an existing list') #<6>
    def test_POST_redirects_to_list_view(self):
        self.assertRedirects(response, '/lists/%d/' % (correct_list.id,)) #<6>
    def test_for_invalid_input_nothing_saved_to_db(self):
        self.assertEqual(Item.objects.count(), 0) #<6>
    def test_for_invalid_input_renders_list_template(self):
        self.assertEqual(response.status_code, 200)
        self.assertTemplateUsed(response, 'list.html') #<6>
    def test_for_invalid_input_passes_form_to_template(self):
        self.assertIsInstance(response.context['form'], ExistingListItemForm) #<7>
    def test_for_invalid_input_shows_error_on_page(self):
        self.assertContains(response, escape(EMPTY_LIST_ERROR)) #<7>
    def test_duplicate_item_validation_errors_end_up_on_lists_page(self):
        self.assertContains(response, expected_error)
        self.assertTemplateUsed(response, 'list.html')
        self.assertEqual(Item.objects.all().count(), 1)
----

<1> Use the Django Test Client

<2> Check the template used.  Then, check each item in the template context:

<3> Check any objects are the right ones, or Querysets have the
    correct items.

<4> Check any forms are of the correct class

<5> Test any template logic:  any `for` or `if` should get a minimal test

<6> For views that handle POST requests, make sure you test both the valid
    case and the invalid case.

<7> Sanity-check that your form is rendered, and its errors are displayed

Why these points?  Skip ahead to <<appendix2,Appendix II>>, and I'll show how
they are sufficient to ensure that our views are still correct if we refactor
them to start using Class-Based Views.

******************************************************************************


== chapter_13 


== chapter_14 


.JavaScript testing notes
*******************************************************************************

* One of the great advantages of Selenium is that it allows you to test that
  your JavaScript really works, just as it tests your Python code.

* There are many JavaScript test running libraries out there.  Qunit is closely
  tied to jQuery, which is the main reason I chose it.  

* Qunit mainly expects you to "run" your tests using an actual web browser.
  This has the advantage that it's easy to create some HTML fixtures that 
  match the kind of HTML your site actually contains, for tests to run against.

* I don't really mean it when I say that JavaScript is awful. It can actually
  be quite fun.  But I'll say it again: make sure you've read
  <<jsgoodparts,JavaScript: The Good Parts>>.

*******************************************************************************


== chapter_15 

.On Spiking and Mocking with JavaScript
*******************************************************************************
Spiking::
    Exploratory coding to find out about a new API, or to explore the
    feasibility   of a new solution.  Spiking can be done without tests.  It's
    a good idea to do your spike on a new branch, and go back to master when
    de-spiking.

Mocking::
    We use mocking in unit tests when we have an external dependency that we
    don't want to actually use in our tests.  A mock is used to simulate the 
    3rd party API.   Whilst it is possible to "roll your own" mocks in
    JavaScript, a mocking framework like Sinon.js provides a lot of helpful
    shortcuts which will make it easier to write (and more importantly, read)
    your tests.

Unit testing Ajax::
    Sinon.js is a great help here. Manually mocking Ajax methods is a real
    pain.

*******************************************************************************


== chapter_16 

.On Mocking in Python
*******************************************************************************

The Mock library::
    Michael Foord (who used to work for the company that spawned
    PythonAnywhere, just before I joined) wrote the excellent "Mock"
    library that's now been integrated into the standard library of Python 3.
    It contains most everything you might need for mocking in Python

The patch decorator::
    `unittest.mock` provides a function called `patch`, which can be used
    to "mock out" any object from the module you're testing.  It's commonly
    used as a decorator on a test method, or even at the class level, where
    it's applied to all the test methods of that class

Mocks are truthy and can mask error::
    Be aware that mocking things out can cause counter-intuitive behaviour
    in `if` statements.  Mocks are truthy, and they can also mask errors,
    because they have all attributes and methods.

Mocking the Django ORM::
    If you want to avoid "touching" the database in your tests, you can
    use Mock to simulate the Django ORM.  I tend to think that's more trouble
    than it's worth. See the "Hot Lava" appendix for more discussion.

Too many mocks are a code smell::
    Overly mocky tests end up very tightly coupled to their implementation.
    Sometimes this is unavoidable.  But, in general, try to find ways
    of organising your code so that you don't need too many mocks.

*******************************************************************************


== chapter_17 

.Fixtures and logging
*******************************************************************************

De-duplicate your FTs, with caution::
    Every single FT doesn't need to test every single part of your application.
    In our case, we wanted to avoid going through the full log-in process for
    every FT that needs an authenticated user, so we used a test fixture to 
    "cheat" and skip that part. You might find other things you want to skip 
    in your FTs.  A word of caution however: functional tests are there to 
    catch unpredictable interactions between different parts of your
    application, so be wary of pushing de-duplication to the extreme.
    
Test fixtures::
    Test fixtures refers to test data that needs to be set up as a precondition
    before a test is run -- often this means populating the database with some
    information, but as we've seen (with browser cookies), it can involve other
    types of preconditions.

Avoid JSON fixtures::
    Django makes it easy to save and restore data from the database in JSON
    format (and others) using the `dumpdata` and `loaddata` management
    commands.  Most people recommend against using these for test fixtures,
    as they are painful to manage when your database schema changes. Use the
    ORM, or a tool like https://factoryboy.readthedocs.org/[factory_boy].

Fixtures also have to work remotely::
    `LiveServerTestCase` makes it easy to interact with the test database 
    using the Django ORM for tests running locally.  Interacting with the 
    database on the staging server is not so straightforward -- one solution
    is Django management commands, as I've shown, but you should explore what
    works for you, and be careful!

Use loggers named after the module you're in::
    The root logger is a single global object, available to any library that's
    loaded in your Python process, so you're never quite in control of it. 
    Instead, follow the `logging.getLogger(__name__)` pattern to get one that's
    unique to your module, but that inherits from a top-level configuration you
    control

Test important log messages::
    As we saw, log messages can be critical to debugging issues in production.
    If a log message is important enough to keep in your codebase, it's
    probably important enough to test.  We follow the rule of thumb that
    anything above `logging.INFO` definitely needs a test.  Using
    `patch.object` on the logger for the module you're testing is one
    convenient way of unit testing it.

*******************************************************************************


== chapter_18 


.Outside-In TDD
*******************************************************************************

Outside-in TDD::
    A methodology for building code, driven by tests, which proceeds by
    starting from the "outside" layers (presentation, GUI), and moving
    "inwards" step-by-step, via view/controller layers, down towards 
    the model layer.  The idea is to drive the design of your code from
    the use to which it is going to be put, rather than trying to anticipate
    requirements from the ground up.

Programming by wishful thinking::
    The outside-in process is sometimes called "programming by wishful
    thinking".  Actually, any kind of TDD involves some wishful thinking. 
    We're always writing tests for things that don't exist yet.

The pitfalls of outside-in::
    Outside-In isn't a silver bullet.  It encourages us to focus on things
    that are immediately visible to the user, but it won't automatically 
    remind us to write other critical tests that are less user-visible; 
    things like security for example. You'll need to remember them yourself.

*******************************************************************************


== chapter_19 

.On the pros and cons of different types of test, and decoupling ORM code
*******************************************************************************

Functional tests::
    * Provide the best guarantee that your application really works correctly,
    from the point of view of the user.
    * But: it's a slower feedback cycle,
    * And they don't necessarily help you write clean code.

Integrated tests (reliant on, eg, the ORM or the Django Test Client)::
    * Are quick to write,
    * Easy to understand,
    * Will warn you of any integration issues,
    * But may not always drive good design (that's up to you!).
    * And are usually slower than isolated tests

Isolated ("mocky") tests::
    * These involve the most hard work.
    * They can be harder to read and understand,
    * But: these are the best ones for guiding you towards better design.
    * And they run the fastest.

Decoupling our application from ORM code::
    When striving to write isolated tests, one of the consequences is that we
    find ourselves forced to remove ORM code from places like views and forms,
    by hiding it behind helper functions or methods.  This can be beneficial in
    terms of decoupling your application from the ORM, but also just because it
    makes your code more readable. As with all things, it's a judgement call as
    to whether the additional effort is worth it in particular circumstances.

*******************************************************************************


== chapter_20 

.Tips on CI and Selenium best practices
*******************************************************************************

Set up CI as soon as possible for your project::
    As soon as your functional tests take more than a few seconds to run,
    you'll find yourself avoiding running them all. Give this job to a CI
    server, to make sure that all your tests are getting run somewhere.

Set up screenshots and HTML dumps for failures::
    Debugging test failures is easier if you can see what the page looked
    at when the failure occurs.  This is particularly useful for debugging
    CI failures, but it's also very useful for tests that you run locally.

Use waits in Selenium tests::
    Selenium's `implicitly_wait` only applies to uses of its `find_element` 
    functions, and even that can be unreliable (it can find an element that's
    still on the old page).  Build a `wait_for` helper function, and alternate
    between actions on the site, and then some sort of wait to see that they've
    taken effect.

Look in to hooking up CI and staging::
    Tests against that use LiveServerTestCase are all very well for dev boxes,
    but the true reassurance comes from running your tests against a real 
    server.  Look into getting your CI server to deploy to your stagin server,
    and run the functional tests against that instead.  It has the side benefit
    of testing your automated deploy scripts.

*******************************************************************************


== chapter_21 

.The Page pattern, and the real exercise for the reader
*******************************************************************************

Apply DRY to your functional tests::
    Once your FT suite starts to grow, you'll find that different tests will
    inevitably find themselves using similar parts of the UI. Try to avoid 
    having constants, like the HTML IDs or classes of particular UI elements
    duplicated between your FTs.

The Page pattern::
    Moving helper methods into a base `FunctionalTest` class can become 
    unwieldy.  Consider using individual Page objects to hold all the
    logic for dealing with particular parts of your site. 

An exercise for the reader::
    I hope you've actually tried this out!  Try to follow the "Outside-In"
    method, and occasionally try things out manually if you get stuck. 
    The real exercise for the reader, of course, is to apply TDD to your
    next project.  I hope you'll enjoy it!

*******************************************************************************


== chapter_22 

.Further reading
*******************************************************************************

Fast Test, Slow Test and Boudaries:: 
    Gary Bernhardt's talks from Pycon 2012
    https://www.youtube.com/watch?v=RAxiiRPHS9k and 
    2013: https://www.youtube.com/watch?v=eOYal8elnZk.  His screencasts at 
    http://www.destroyallsoftware.com are also well worth a look.

Ports and Adapters:: 
    Steve Freeman and Nat Pryce wrote about this in <<GOOSGBT, their book>>.
    You can also catch a good discussion of the idea in this talk:
    http://vimeo.com/83960706. There's also
    http://blog.8thlight.com/uncle-bob/2012/08/13/the-clean-architecture.html[Uncle
    Bob's blog], and 
    http://alistair.cockburn.us/Hexagonal+architecture[Alistair Cockburn's
    site].

Hot Lava::
    Casey Kinsey's memorable warning about avoiding the database whenever
    you can: https://www.youtube.com/watch?v=bsmFVb8guMU

Inverting the Pyramid::
    The idea that projects end up with too great a ratio of slow, high-level
    tests to unit tests, and a visual metaphor for the effort to invert that
    ratio: http://watirmelon.com/tag/testing-pyramid/

Integrated tests are a scam::
    J.B. Rainsberger has a famous rant about the way integrated tests will
    ruin your life, http://blog.thecodewhisperer.com/2010/10/16/integrated-tests-are-a-scam/[here].
    Watch the video presentation 
    http://www.infoq.com/presentations/integration-tests-scam[here] or 
    http://vimeo.com/80533536[here] (there are two videos available choose,
    neither has perfect cinematography). Then check out a couple of 
    follow-up posts, particularly 
    http://www.jbrains.ca/permalink/using-integration-tests-mindfully-a-case-study[this
    defence of acceptance tests] (what I call functional tests), and
    http://www.jbrains.ca/permalink/part-2-some-hidden-costs-of-integration-tests[this
    analysis of how slow tests kill productivity]

*******************************************************************************

