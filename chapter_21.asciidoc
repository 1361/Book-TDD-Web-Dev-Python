Fast tests, slow tests and Hot Lava
-----------------------------------

[quote, 'https://www.youtube.com/watch?v=bsmFVb8guMU[Casey Kinsey]']
______________________________________________________________
The database is Hot Lava!
______________________________________________________________

Time to let you in on a little secret.  Almost none of the "unit" tests 
in this book are "true" unit tests.  Strictly speaking, you should
call them "integrated" tests, because they either rely on the database, or
they use the Django Test Client, which does too much magic with the middleware
layers that sit between requests, responses and view functions.

TDD veterans say you should strive to write unit tests wherever possible,
instead of writing integrated tests.  In this chapter, I'd like to talk about
why they say that, and try and give you some idea of when you can get away with
muddling through with integrated tests, and when it's worth striving for more
"pure" unit tests..

If you'll forgive me the pretentious philosophical terminology, I'm going to
follow a Hegelian dialectical structure: 

* The Thesis: the case for "purist" unit tests that are fast.

* The Antithesis: the pitfalls of "pure" unit tests.

* The Synthesis: a discussion of ideal best practices like "Ports and Adapters"
  or "Functional Core, Imperative Shell", and how to figure out when they're
  worth striving for.


.Terminology: different types of test
******************************************************************************

Isolated tests ("pure" unit tests) vs integrated tests:: 
    The primary purpose of a unit test should be to verify the correctness
    of the logic of your application.  
    An 'isolated' test is one that tests exactly one chunk of code, and whose
    success or failure does not depend on any other external code. This is what
    I call a "pure" unit test:  a test for a single function, for example,
    written in such a way that only that function can make it fail.  If the
    function depends on another system, and breaking that system breaks our
    test, we have an 'integrated' test. That system could be an external
    system, like a database, but it could also be another function which we
    don't control.  In either case, if breaking the system makes our test fail,
    our test is not properly isolated, it is not a "pure" unit test.  That's
    not necessarily a bad thing, but it may mean the test is doing two jobs at
    once.

Integration tests::
    An integration test checks that the code you control is integrated
    correctly with some external system which you don't control. One way of
    writing 'integration' tests is by using an 'integrated' test which actually
    uses that external system, but another way is to use mocks, as we did in
    the Persona chapters.

System tests::
    If an integration test checks the integration with one external system,
    a system test checks the integration of multiple systems in your
    application -- for example, checking that we've wired up our database,
    static files, our server config, together in such a way that they all work.
    
Functional tests and Acceptance tests::
    An acceptance test is meant to test that our system works from the point
    of view of the user ("would the user accept this behaviour?").  It's 
    hard to write an acceptance test that's not a full-stack, end-to-end test.
    We've been using our functional tests to play the role of both acceptance
    tests and system tests.
    
******************************************************************************


Thesis: Unit tests should be pure and fast
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Other things being equal, the faster your unit tests run, the better.  To a 
lesser extent, the faster 'all' your tests run, the better.


Faster tests mean faster development
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

I've outlined the TDD test/code cycle in this book.  You've started to get a 
feel for the TDD workflow, the way you flick between writing tiny amounts of
code, and running your tests.  You end up running your unit tests several times
a minute, and your functional tests several times a day. So, naturally, the
longer they take, the more time you spend waiting for your tests, and that 
will slow down your development.


Slow tests don't get run as often, which causes bad code
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The follow-up argument is that, if your test suite is slow, then running it
becomes annoying.  As a developer, the danger is that you'll start to avoid
running your tests, which may lead to bugs getting through, or it may lead
to programmers being shy of refactoring the code, since they know that any
refactor will mean having to wait ages while all the tests run. In either
case, bad code can be the result.


Good unit tests should be isolated
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The idealised unit test test exactly one thing -- this is why we strive to
have a single assertion per test.  If your unit test actually has several
dependencies, if the function under test calls other functions, and we end
up relying on their effects, our unit tests are no longer isolated.

Another way of thinking of it is: in the idealised unit test setup, introducing
a bug should cause exactly one unit test to fail, and that unit test should 
tell you exactly where the problem is.  If introducing a bug in a function
causes several other tests to fail, because they rely on that function 
indirectly, then it's harder to debug the problem.


Don't take it from me
^^^^^^^^^^^^^^^^^^^^^

Gary Bernhardt, a man with far more experience of testing than me, put these
points eloquently in a talk called
https://www.youtube.com/watch?v=RAxiiRPHS9k[Fast Tests, Slow test]. I encourage
you to watch it.  

You should also check out his follow-up talk, 
https://www.youtube.com/watch?v=eOYal8elnZk[Boundaries], in which he expands on
the concepts of test isolation, and how it fits with trying to adopt
a more functional paradigm in the core business logic of your application.


The problems with "pure" unit tests
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

That all sounds convincing, right?  The problem comes when striving for pure,
fast unit tests leads to using a lot of mocks.

Here's an illustration. Imagine our site does a bit of setup for each
user: it makes a temp folder for them, and then sets a flag on their user
profile:

Imagine this:

[source,python]
----

def _setup_temp_storage(user):
    os.makedirs(TEMPDIR + '/' + user.username)

def setup_user_environment(user):
    _setup_temp_storage(user)
    profile = user.get_profile()
    profile.environment_setup = True
    profile.save()
----

Here's the kind of test I'm inclined to write:

[source,python]
----
def test_sets_up_temp_storage_and_sets_flag_on_profile(self):
    user = User.objects.create(username='jim')
    setup_user_environment(user)
    self.assertTrue(os.path.exists(TEMPDIR + '/' + 'jim')
    self.assertTrue(user.get_profile().environment_setup)

def tearDown(self):
    cleanup_tempdir()
----


Clearly breaking all the rules of "pure" unit tests there -- our test touches
not just the database, but the filesytem as well. Oh no!


Mocky tests can be harder to work with
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

But consider what happens if we naively try and write a "pure" unit test, with
mocks:


[source,python]
----
def test_calls_setup_temp_storage_and_sets_flag_on_profile(self):
    mock_user = Mock()
    with patch('myapp._setup_temp_storage') as mock_setup_temp_storage:
        setup_user_environment(mock_user)
    mock_setup_temp_storage.assert_called_once_with(mock_user)
    mock_profile = mock_user.get_profile.return_value
    self.assertEqual(mock_profile.environment_setup, True)
----

Well, I would argue that, firstly, you have a much less readable test there,
and it's also a test that's very closely coupled to the implementation.  It
discourages refactoring, because something as simple as changing the name of
the helper method `_setup_temp_storage` involves changing the test code in 4
places -- three of which (eg `mock_setup_temp_storage`) won't be found by
automated refactoring tools.


The problem of the "disconnected" mocky test
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

But, secondly: imagine I change `_setup_temp_storage` to take a username
instead of a user. I go and find its unit tests and change them, then change
its implementation. What will happen next is that my "impure" unit test for
`setup_user_environment` would break, because it uses the real function, and so
that's my reminder to change the place it gets used. 

In contrast, in the "fast" test, `setup_user_environment` is mocked, so 'that
test will still pass', even though my code is broken.  If we were using a 
statically typed language like Java, the compiler would warn us, but in Python,
it's up to us to catch this sort of problem.

So there's a danger, if you just dive in with mocks everywhere, that your 
tests won't pick up on bugs in your application that are due to the way
the individual components are integrated.




Synthesis: Functional core, Imperative shell... if it works for you
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you've watched Gary's second talk, called 
"https://www.youtube.com/watch?v=eOYal8elnZk[Boundaries]",  you'll start to
see a potential way out of our thesis/antithesis dilemma.

The problem of choosing between mocky tests and integrated tests 
only really occurs at "boundaries" -- points of interaction between our
code and external systems, like a database or a filesystem.

So the solution is to try and keep the boundaries as separate as possible from
the core of our application.  We can judiciously use a combination of mocks and
integrated tests, maybe even functional tests, to test our code at the
boundaries, but keep the "core" of our application -- the business logic --
free of direct interactions with the database or the filesystem.

Steve Freeman and Nat Pryce, in their book <<GOOSGBT, Growing Object-Oriented
Software, Guided By Tests>>, call their approach to this "Ports and
Adapters" (see <<ports-and-adapters>>).

[[ports-and-adapters]]
.Ports and Adapters (diagram by Nat Pryce)
image::images/ports-and-adapters-architecture.svg[Illustration of ports and adapaters architecture, with isolated core and integration points]

You can also see
http://blog.8thlight.com/uncle-bob/2012/08/13/the-clean-architecture.html[Uncle
Bob's perspective on his blog], and 
http://alistair.cockburn.us/Hexagonal+architecture[Alistair Cockburn coining
the term Hexagonal Architecture].

Gary pushes this further, recommending an architecture he calls "Functional
Core, Imperative Shell", whereby the "shell" of the application, the place
where interaction with boundaries happens, follows the imperative programming
pattern, and can be tested by integrated tests, acceptance tests, or even
(gasp!) not at all, if it's kept minimal enough. But the core of the
application is actually written following the functional programming paradigm
(complete with the "no side-effects" corollary), which actually allows fully
isolated, "pure" unit tests, entirely without mocks.


Either way, the end result should be an application that's (say) 80% core and
20% boundaries, 80% unit tests and 20% integrated tests.  This is sometimes
described as trying to create a
http://watirmelon.com/tag/testing-pyramid/["testing pyramid"].


A health warning: simple examples in a book don't always reflect real life
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The problem is that, while the FCIS architecture sounds like a great idea
for an app of moderate complexity, the example we used in this book was
way to simple to really demonstrate the concept.

A simple Django app is basically a thin wrapper around a CRUD system for a
database.  It doesn't feel like there's a large enough "core" that's worth
extracting and isolating from its boundaries.  But in a more complex system
it's something that's definitely worth exploring.  It probably depends on
your ration of "boundaries" to "business logic".

On the biggest project I've ever worked on, PythonAnywhere, almost everything
we do interacts with boundaries, one or more of: the filesystem, the database,
websockets, Paypal, Dropbox, github, pypi, linux chroots and cgroups, CRON,
Nginx and uWSGI, and many more.  So it's never seemed worth-while to create a
separate business logic layer. But most apps are not PythonAnywhere.

You can verify the correctness of your application using integrated tests,
as we've done throughout the book. But remember, the tests we write in
test-driven development are meant to do two jobs: not just verify correctness,
but also help to drive good design.  Striving to write good, isolated tests
should help to drive a clean, modular design.  If you rely too much on 
integrated tests, you may not get the benefits of improved design in the
same way.

A wider discussion of these issues is beyond the scope of this book and the 
wisdom of its writer, but here are a few further reading sources I encourage
you to take a look at.


.Further reading
*******************************************************************************

Fast Test, Slow Test and Boudaries:: 
    Gary Bernhardt's talks from Pycon 2013
    https://www.youtube.com/watch?v=RAxiiRPHS9k and 
    https://www.youtube.com/watch?v=eOYal8elnZk.  His screencasts at 
    http://www.destroyallsoftware.com are also well worth a look.

Ports and Adapters:: 
    Steve Freeman and Nat Pryce wrote about this in <<GOOSGBT, their book>>.
    You can also catch a good discussion of the idea in this talk:
    http://vimeo.com/83960706. There's also
    http://blog.8thlight.com/uncle-bob/2012/08/13/the-clean-architecture.html[Uncle
    Bob's blog], and 
    http://alistair.cockburn.us/Hexagonal+architecture[Alistair Cockburn's
    site].

Hot Lava::
    Casey Kinsey's memorable warning about avoiding the database whenever
    you can: https://www.youtube.com/watch?v=bsmFVb8guMU


Integrated tests are a scam::
    J.B. Rainsberger has a famous rant about the way integrated tests will
    ruin your life, http://blog.thecodewhisperer.com/2010/10/16/integrated-tests-are-a-scam/[here].
    Watch the video presentation 
    http://www.infoq.com/presentations/integration-tests-scam[here] or 
    http://vimeo.com/80533536[here] (there are two videos available choose,
    neither has perfect cinematography). Then check out a couple of 
    follow-up posts, particularly 
    http://www.jbrains.ca/permalink/using-integration-tests-mindfully-a-case-study[this
    defence of acceptance tests] (what I call functional tests), and
    http://www.jbrains.ca/permalink/part-2-some-hidden-costs-of-integration-tests[this
    analysis of how slow tests kill productivity]

Inverting the Pyramid::

    http://watirmelon.com/tag/testing-pyramid/

*******************************************************************************

