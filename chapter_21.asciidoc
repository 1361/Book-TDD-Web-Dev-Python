Fast tests, slow tests and Hot Lava
-----------------------------------

Time to let you in on a little secret.  Almost none of the "unit" tests 
in this book are "true" unit tests.  Purists would call them "integration"
tests, because they either rely on the database, or they use the Django 
Test Client, which does too much magic with the middleware layers that sit
between requests, responses and view functions.

[quote, 'https://www.youtube.com/watch?v=bsmFVb8guMU[Casey Kinsey]']
______________________________________________________________
The database is Hot Lava!
______________________________________________________________

Purists say you should strive to write unit tests wherever possible, instead
of writing integration tests.  In this chapter, I'd like to talk about why they
say that, and try and give you some idea of when you can get away with a 
"pragmatic" appraoch, and when it's worth striving for purity.

If you'll forgive me the pretentious philosophical terminology, I'm going to
follow a Hegelian dialectical structure: 

* The thesis: the case for "purist" unit tests that are fast

* The antithesis: the pitfalls of "pure" unit tests

* The synthesis: a discussion of ideal best practices like "Ports and Adapters"
  or "Functional Core, Imperative Shell", and how to figure out whether they're
  worth striving for.


Thesis: Unit tests should be pure and fast
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Other things being equal, the faster your unit tests run, the better.  To a 
lesser extent, the faster 'all' your tests run, the better.


Faster tests mean faster development
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

I've outlined the TDD test/code cycle in this book.  You've started to get a 
feel for the TDD workflow, the way you flick between writing tiny amounts of
code, and running your tests.  You end up running your unit tests several times
a minute, and your functional tests several times a day. So, naturally, the
longer they take, the more time you spend waiting for your tests, and that 
will slow down your development.


Slow tests don't get run as often, which causes bad code
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The follow-up argument is that, if your test suite is slow, then running it
becomes annoying.  As a developer, the danger is that you'll start to avoid
running your tests, which may lead to bugs getting through, or it may lead
to programmers being shy of refactoring the code, since they know that any
refactor will mean having to wait ages while all the tests run. In either
case, bad code can be the result.


Good unit tests should be isolated
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The idealised unit test test exactly one thing -- this is why we strive to
have a single assertion per test.  If your unit test actually has several
dependencies, if the function under test calls other functions, and we end
up relying on their effects, our unit tests are no longer isolated.

Another way of thinking of it is: in the idealised unit test setup, introducing
a bug should cause exactly one unit test to fail, and that unit test should 
tell you exactly where the problem is.  If introducing a bug in a function
causes several other tests to fail, because they rely on that function 
indirectly, then it's harder to debug the problem.


Don't take it from me
^^^^^^^^^^^^^^^^^^^^^

Gary Bernhardt, a man with far more experience of testing than me, put these
points eloquently in a talk called
https://www.youtube.com/watch?v=RAxiiRPHS9k[Fast Tests, Slow test]. I encourage
you to watch it.  

You should also check out his follow-up talk, 
https://www.youtube.com/watch?v=eOYal8elnZk[Boundaries], in which he expands on
the concepts of test isolation, and how it fits with trying to adopt
a more functional paradigm in the core business logic of your application.


The problems with "pure" unit tests
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

That all sounds convincing, right?  The problem, in my view, comes when
striving for pure, fast unit tests leads to using a lot of mocks.

Here's an illustration. Imagine our site does a bit of setup for each
user: it makes a temp folder for them, and then sets a flag on their user
profile:

Imagine this:

[source,python]
----
def _setup_temp_storage(user):
    os.makedirs('/tmp/' + user.username)

def setup_user_environment(user):
    _setup_temp_storage(user)
    profile = user.get_profile()
    profile.environment_setup = True
    profile.save()
----

Here's the kind of test I'm inclined to write:

[source,python]
----
def test_sets_up_temp_storage_and_sets_flag_on_profile(self):
    user = User.objects.create(username='jim')
    setup_user_environment(user)
    self.assertTrue(os.path.exists('/tmp/jim')
    self.assertTrue(user.get_profile().environment_setup)
----

Clearly breaking all the rules of "pure" unit tests there -- my test touches
not just the database, but the filesytem as well. Oh no!


Mocky tests are harder to work with
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

But consider what happens when you try and write a "pure" unit test, with
mocks:


[source,python]
----
def test_calls_setup_temp_storage_and_sets_flag_on_profile(self):
    mock_user = Mock()
    with patch('myapp._setup_temp_storage') as mock_setup_temp_storage:
        setup_user_environment(mock_user)
    mock_setup_temp_storage.assert_called_once_with(mock_user)
    mock_profile = mock_user.get_profile.return_value
    self.assertEqual(mock_profile.environment_setup, True)
----

Well, I would argue that, firstly, you have a much less readable test there,
and it's also a test that's very closely coupled to the implementation.  It
discourages refactoring, because something as simple as changing the name of
the helper method `_setup_temp_storage` involves changing the test code in 4
places -- three of which (eg `mock_setup_temp_storage`) won't be found by
automated refactoring tools.


The problem of the "disconnected" mocky test
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

But, secondly: imagine I change `_setup_temp_storage` to take a username
instead of a user. I go and find its unit tests and change them, then change
its implementation. What will happen next is that my "impure" unit test for
`setup_user_environment` would break, because it uses the real function, and so
that's my reminder to change the place it gets used. 

In contrast, in the "fast" test, `setup_user_environment` is mocked, so 'that
test will still pass', even though my code is broken.  If we were using a 
strongly typed language like Java, the compiler would warn us, but in Python,
it's up to us to catch this sort of problem.

This is why perfectly isolated, mocked tests can never give you full confidence
that your application works, and some sort of integration test is always
necessary.


Synthesis: Functional core, Imperative shell... if it works for you
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you've watched Gary's second talk, called 
"https://www.youtube.com/watch?v=eOYal8elnZk[Boundaries]",  you'll start to
see a potential way out of our thesis/antithesis dilemma.

The problem of choosing between "mocky" tests and "integrationey" tests 
only really occurs at "boundaries" -- points of interaction between our
code and external systems, like a database or a filesystem.

What Gary calls "Functional Core, Imperative Shell", which is also known as
"Ports and Adapters" or "Hexagonal Architecture", is a solution where we try
and keep the boundaries as separate as possible from the core of our
application.  We can judiciously use a combination of mocks and integration
tests, maybe even functional tests, to test our code at the boundaries, but
keep the "core" of our application -- the business logic -- free of direct
interactions with the database or the filesystem.

[[ports-and-adapters]]
.Ports and Adapters (diagram by Nat Pryce)
image::images/ports-and-adapters-architecture.svg[Illustration of ports and adapaters architecture, with isolated core and integration points]

You can find a write-up of "Ports and Adapters" in Steve Freeman and Nat
Pryce's <<GOOSGBT, Growing Object-Oriented Software, Guided By Tests>>.
You can also see 
http://blog.8thlight.com/uncle-bob/2012/08/13/the-clean-architecture.html[Uncle
Bob's perspective on his blog], and 
http://alistair.cockburn.us/Hexagonal+architecture[Alistair Cockburn coining
the term Hexagonal Architecture].

In that perfect world, we can start to build tests that don't need any
mocks, especially if we follow functional programming principles -- the
less state there is in our code, the easier it is to test.

The end result should be an application that's (say) 80% core and 20% shell,
80% unit tests and 20% integration tests.


Why I didn't write any "pure" unit tests: my app was too simple
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

And that's all great, in theory.  But sometimes it feels unnecessary.
Certainly for our tiny little To-Do lists app, building an abstraction layer
around the Django ORM felt like too much work.

In a more complex system, it may become more worth-while, depending on what
your ratio of "boundaries" to "business logic" is.

On the biggest project I've ever worked on, PythonAnywhere, almost everything you do
will interact with one or more of: the filesystem, the database, Tornado +
websockets, Paypal, Dropbox, github, pypi, linux chroots and cgroups, CRON,
Nginx and uWSGI, and many more.  So it's never seemed worth-while to create
a separate business logic layer... And you should be aware that that experienc
has biased me!

But the applications you work on may well be different.  A simple Django app
is basically just a thin wrapper around a CRUD system for a database, but once
it gets more complex, you should start to think carefully about how to separate
a core from its integration points.

A wider discussion of these issues is beyond the scope of this book
unfortunately, but I hope you'll take a peek at some of the further reading
resources I've mentioned.


.Further reading
*******************************************************************************
Fast Test, Slow Test and Boudaries:: 
    Gary Bernhardt's talks from Pycon 2013
    https://www.youtube.com/watch?v=RAxiiRPHS9k and 
    https://www.youtube.com/watch?v=eOYal8elnZk.  His screencasts at 
    http://www.destroyallsoftware.com are also well worth a look.

Hot Lava::
    Casey Kinsey's memorable warning about avoiding the database whenever
    you can: https://www.youtube.com/watch?v=bsmFVb8guMU

Ports and Adapters:: 
    Steve Freeman and Nat Pryce wrote about this in <<GOOSGBT, their book>>.
    You can also catch a good discussion of the idea in this talk:
    http://vimeo.com/83960706, as well as on
    http://blog.8thlight.com/uncle-bob/2012/08/13/the-clean-architecture.html[Uncle
    Bob's blog], and on
    http://alistair.cockburn.us/Hexagonal+architecture[Alistair Cockburn's
    site]

*******************************************************************************

