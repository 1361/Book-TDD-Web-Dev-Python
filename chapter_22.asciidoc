[[hot-lava-chapter]]
Fast tests, slow tests and Hot Lava
-----------------------------------


NOTE: The book has been upgraded to Django 1.7 beta
overnight. If you started out on version 1.6, you
can upgrade using
+pip install --upgrade https://www.djangoproject.com/download/1.7b1/tarball/+

[quote, 'https://www.youtube.com/watch?v=bsmFVb8guMU[Casey Kinsey]']
______________________________________________________________
The database is Hot Lava!
______________________________________________________________

Right up until <<isolation-chapter>>, almost all of the "unit" tests in
the book were 'integrated' tests, because they either rely on the database, or
they use the Django Test Client, which does too much magic with the middleware
layers that sit between requests, responses and view functions.

There is an argument that an true unit test should always be isolated, because
it's meant to test a single unit of software.

TDD veterans say you should strive to write "pure", isolated unit tests
wherever possible, instead of writing integrated tests.  Being merely a young
whippersnapper myself, I'm only part way towards realising the full truth of
this prescription. But you should understand that as being a sign of my own
inexperience and hopelessly contrary nature, rather than any sort of indicator
of truth.

In this chapter, I'd like to talk about why they say that, and try and give you
some idea of when you can get away with muddling through with integrated tests
(which I confess I do a lot of!), and when it's worth striving for more "pure"
unit tests..


.Terminology: different types of test
******************************************************************************

Isolated tests ("pure" unit tests) vs integrated tests:: 
    The primary purpose of a unit test should be to verify the correctness
    of the logic of your application.  
    An *isolated* test is one that tests exactly one chunk of code, and whose
    success or failure does not depend on any other external code. This is what
    I call a "pure" unit test:  a test for a single function, for example,
    written in such a way that only that function can make it fail.  If the
    function depends on another system, and breaking that system breaks our
    test, we have an *integrated* test. That system could be an external
    system, like a database, but it could also be another function which we
    don't control.  In either case, if breaking the system makes our test fail,
    our test is not properly isolated, it is not a "pure" unit test.  That's
    not necessarily a bad thing, but it may mean the test is doing two jobs at
    once.

Integration tests::
    An integration test checks that the code you control is integrated
    correctly with some external system which you don't control. 
    'Integration' tests are typically also 'integrated' tests. 

System tests::
    If an integration test checks the integration with one external system,
    a system test checks the integration of multiple systems in your
    application -- for example, checking that we've wired up our database,
    static files, our server config, together in such a way that they all work.
    
Functional tests and Acceptance tests::
    An acceptance test is meant to test that our system works from the point
    of view of the user ("would the user accept this behaviour?").  It's 
    hard to write an acceptance test that's not a full-stack, end-to-end test.
    We've been using our functional tests to play the role of both acceptance
    tests and system tests.
    
******************************************************************************


If you'll forgive me the pretentious philosophical terminology, I'm going to
follow a Hegelian dialectical structure: 
 
* The Thesis: the case for "pure" unit tests that are fast

* The Antithesis: some of the risks associated with a (naive) pure unit testing
  approach.

* The Synthesis: a discussion of best practices like "Ports and Adapters"
  or "Functional Core, Imperative Shell", and of just what it is that we want
  from our tests, anyway.


Thesis: Unit tests are superfast and good besides that
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

One of the things you often hear about unit tests is that they're much faster.
I don't think that's actually the primary benefit of unit tests, but it's worth
exploring the theme of speed.


Faster tests mean faster development
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Other things being equal, the faster your unit tests run, the better.  To a 
lesser extent, the faster 'all' your tests run, the better.

I've outlined the TDD test/code cycle in this book.  You've started to get a 
feel for the TDD workflow, the way you flick between writing tiny amounts of
code, and running your tests.  You end up running your unit tests several times
a minute, and your functional tests several times a day. 

So, on a very basic level, the longer they take, the more time you spend
waiting for your tests, and that will slow down your development.  But
there's more to it than that.


The holy Flow state
^^^^^^^^^^^^^^^^^^^

Thinking sociology for a moment, we programmers have our own culture, and
our own tribal religion in way. It has many congregations within it such as the
cult of TDD to which you are now initiated.  There the followers of vi and the
heretics of emacs. But one thing we all agree on, one particular spiritual
practice, our own transcendental meditation, is the holy flow state.  That
feeling of pure focus, of concentration, where hours pass like no time at all,
where code flows naturally from our fingers, where problems are just tricky
enough to be interesting but not so hard that they defeat us...

There is absolutely no hope of achieving flow if you spend your time waiting
for a slow test suite to run.  Anything longer than a few seconds and you're
going to let your attention wander, you context-switch, and the flow state is
gone.  And the flow state is a fragile dream. Once it's gone, it takes at
least 15 minutes to live again.


Slow tests don't get run as often, which causes bad code
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If your test suite is slow and ruins your concentration, the danger is that
you'll start to avoid running your tests, which may lead to bugs getting
through. Or, it may lead to our being shy of refactoring the code,
since we know that any refactor will mean having to wait ages while all the
tests run. In either case, bad code can be the result.


We're fine now, but integrated tests get slower over time
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

You might be thinking, OK, but our test suite has lots of integrated
tests in it - over 50 of them, and it only takes 0.2 seconds to run.

But remember, we've got a very simple app. Once it starts to get more
complex, as your database grows more and more tables and columns, integrated
tests will get slower and slower.  Having Django reset the database between
each test will take longer and longer.


Don't take it from me
^^^^^^^^^^^^^^^^^^^^^

Gary Bernhardt, a man with far more experience of testing than me, put these
points eloquently in a talk called
https://www.youtube.com/watch?v=RAxiiRPHS9k[Fast Tests, Slow test]. I encourage
you to watch it.  


And unit tests drive good design
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

But perhaps more importantly than any of this, remember the lesson from
<<isolation-chapter>>.  Going through the process of writing good, isolated
unit tests can help us drive out better designs for our code, by forcing us
to identify dependencies, in a way that integrated tests don't.



The problems with "pure" unit tests
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

All of this comes with a huge "but". Writing isolated united tests comes with
its own hazards, particularly if, like you or I, we are not yet advanced
TTD'ers.


Isolated tests can be harder to read and write
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Cast your mind back to the first isolated unit test we wrote.  Wasn't it ugly?
Admittedly, things improved when we refactored things out into the forms, but
imagine if we hadn't followed through?  We'd have been left with a very ugly
test in our codebase.  And even the final version of the tests we ended up
with contain some pretty ugly bits.


Isolated tests don't automatically test integration
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

As we saw a little later on, isolated tests by their nature only test the
unit under test, in isolation.  They won't test the integration between 
your units.

This problem is well known, and there are ways of mitigating it. But, as
we saw, those mitigations involve a fair bit of hard work on the part of
the programmer -- you need to remember to keep track of the interfaces
between your units, to identify the implicit contract that each component
needs to honour, and you need to write tests for those contracts as well
as for the internal functionality of your unit.


Unit tests seldom catch unexpected bugs
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Isolated tests won't remind you when you forgot to create a database migration.
They won't tell you when the middleware layer is doing some clever HTML-entity
escaping that's interfering with the way your data is rendered.


Mocky tests can become closely tied to implementation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

And finally, mocky tests can become closely coupled with the implementation.
If you choose to use `List.objects.create()` to build your objects but your
mocks are expecting you to use `List()`, you'll get failing tests even though
the actual effect of the code would be the same.

Notice, though, that this is mainly a problem when you're mocking out a 3rd
party API, like Django.  When you're mocking out your own code, you control
the implementation much more closely, so it's less likely to be a problem.

There's potentially another risk

- isolated tests can solidify around code, make it harder to change.  


But, purist will come back and say, all that stuff can be mitigated, you
just need to get better at writing isolated tests, and, remember the holy
flow state?  won't somebody 'please' think of the flow state?

so where are we.


Synthesis: What do we want from our tests, anyway?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Correctness

* at both low level and high level
* help us think through edge cases
* prevent regressions
* drive development from POV of user

Clean code

* outside-in, YAGNI
* encourage refactoring

Workflow

* stay in flow state as much as poss
* provide fast feedback -- we want to know asap when we introduce a bug
* maintainability -- we don't want our tests to become a barrier to change



Architectural solutions
^^^^^^^^^^^^^^^^^^^^^^^

So it sounds like, other things being equal, having more unit tests
and fewer integrated tests would help get us faster feedback cycles
and drive a better application design.  


This is sometimes described as trying to create a
http://watirmelon.com/tag/testing-pyramid/["testing pyramid"].


But how do we avoid the risk of our test suite becoming less reliable at
finding bugs, as we make it more isolated?


Ports and adapters / Hexagonal architecture
+++++++++++++++++++++++++++++++++++++++++++

Integrated tests are most useful at the 'boundaries' of a system -- at
the points where our code integrates with external systems, like a
database, filesystem, or UI components.

similarly, the downsides of mocks are at their worst at the boundaries.

Conversely, code at the 'core' of our application -- code that's purely
concerned with our business domain and business rules, code that's 
entirely under our control -- this code has less need for integrated
tests, since we control and understand all of it.

So one way of getting what we want is to try and minimise the amount
of our code that has to deal with boundaries, test our core business
logic with isolated tests and test our integration points with integrated
tests.

Steve Freeman and Nat Pryce, in their book <<GOOSGBT, Growing Object-Oriented
Software, Guided By Tests>>, call this approach "Ports and Adapters" (see
<<ports-and-adapters>>).

[[ports-and-adapters]]
.Ports and Adapters (diagram by Nat Pryce)
image::images/ports-and-adapters-architecture.svg[Illustration of ports and adapaters architecture, with isolated core and integration points]

You can also see
http://blog.8thlight.com/uncle-bob/2012/08/13/the-clean-architecture.html[Uncle
Bob's perspective on his blog], and you should also have a look at 
http://alistair.cockburn.us/Hexagonal+architecture[Alistair Cockburn coining
the term Hexagonal Architecture] to describe this pattern (I think he may
be the person who originally came up with "Ports and Adapters" actually).



Functional Core, Imperative Shell
+++++++++++++++++++++++++++++++++

Gary Bernhard pushes this further, recommending an architecture he calls
"Functional Core, Imperative Shell", whereby the "shell" of the application,
the place where interaction with boundaries happens, follows the imperative
programming paradigm, and can be tested by integrated tests, acceptance tests,
or even (gasp!) not at all, if it's kept minimal enough. But the core of the
application is actually written following the functional programming paradigm
(complete with the "no side-effects" corollary), which actually allows fully
isolated, "pure" unit tests, 'entirely without mocks'.

Check out Gary's presentation titled
"https://www.youtube.com/watch?v=eOYal8elnZk[Boundaries]" for more on this
approach.



Conclusion: 
~~~~~~~~~~~

<wrap it all up nicely, point to further reading>


.Further reading
*******************************************************************************

Fast Test, Slow Test and Boudaries:: 
    Gary Bernhardt's talks from Pycon 2012
    https://www.youtube.com/watch?v=RAxiiRPHS9k and 
    2013: https://www.youtube.com/watch?v=eOYal8elnZk.  His screencasts at 
    http://www.destroyallsoftware.com are also well worth a look.

Ports and Adapters:: 
    Steve Freeman and Nat Pryce wrote about this in <<GOOSGBT, their book>>.
    You can also catch a good discussion of the idea in this talk:
    http://vimeo.com/83960706. There's also
    http://blog.8thlight.com/uncle-bob/2012/08/13/the-clean-architecture.html[Uncle
    Bob's blog], and 
    http://alistair.cockburn.us/Hexagonal+architecture[Alistair Cockburn's
    site].

Hot Lava::
    Casey Kinsey's memorable warning about avoiding the database whenever
    you can: https://www.youtube.com/watch?v=bsmFVb8guMU


Integrated tests are a scam::
    J.B. Rainsberger has a famous rant about the way integrated tests will
    ruin your life, http://blog.thecodewhisperer.com/2010/10/16/integrated-tests-are-a-scam/[here].
    Watch the video presentation 
    http://www.infoq.com/presentations/integration-tests-scam[here] or 
    http://vimeo.com/80533536[here] (there are two videos available choose,
    neither has perfect cinematography). Then check out a couple of 
    follow-up posts, particularly 
    http://www.jbrains.ca/permalink/using-integration-tests-mindfully-a-case-study[this
    defence of acceptance tests] (what I call functional tests), and
    http://www.jbrains.ca/permalink/part-2-some-hidden-costs-of-integration-tests[this
    analysis of how slow tests kill productivity]

Inverting the Pyramid::

    http://watirmelon.com/tag/testing-pyramid/

*******************************************************************************

